{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7206, 0.7671, 0.1026],\n",
       "        [0.8758, 0.5615, 0.4104],\n",
       "        [0.4095, 0.5284, 0.2358],\n",
       "        [0.7057, 0.8597, 0.5278]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7206, 0.7671, 0.1026],\n",
      "        [0.8758, 0.5615, 0.4104],\n",
      "        [0.4095, 0.5284, 0.2358],\n",
      "        [0.7057, 0.8597, 0.5278]])\n"
     ]
    }
   ],
   "source": [
    "print(x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2.],requires_grad=True)\n",
    "b = torch.tensor([6.],requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 2*a**2-4*b**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.]) tensor([-48.])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad.data,b.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Use Deep Neural Networks (DNNs) in Machine Learning\n",
    "\n",
    "## Universal Approximation Theorem\n",
    "DNNs have the ability to approximate any continuous function given sufficient data and computational resources.\n",
    "\n",
    "## Advantages of DNNs\n",
    "DNNs can leverage large amounts of training data, which allows them to perform better than many classical machine learning algorithms.\n",
    "\n",
    "# Supervised Learning (Classifier, Regressor)\n",
    "\n",
    "## How It Works\n",
    "In supervised learning, a neural network accepts an input as a vector and predicts an output. This can be used for tasks such as classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DNN](DNN.png)\n",
    "\n",
    "The weighted sum formula for a perceptron involves multiplying each input feature \\( x_i \\) by its corresponding weight \\( w_i \\) and then summing these products. For \\( n \\) input features, the formula can be written as:\n",
    "\n",
    "z = is weighted sum\n",
    "\n",
    "z = w1x1 + w1x2 + ... + wnxn = XTW\n",
    "\n",
    "this z is further processed by an activation function to produce the output.\n",
    "\n",
    "![Perceptron](Perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERCEPTRON WITH PYTORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wSum(X,W):\n",
    "    h = torch.tensor(X, dtype=torch.float32)\n",
    "    z = torch.matmul(W,h)\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 10\n",
    "n = 1000\n",
    "X = np.random.rand(n,inputDim)\n",
    "y = np.random.randint(0,2,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor(np.random.uniform(0,1,inputDim),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dot : expected both vectors to have same dtype, but found Double and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mwSum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m, in \u001b[0;36mwSum\u001b[1;34m(X, W)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwSum\u001b[39m(X,W):\n\u001b[0;32m      2\u001b[0m     h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m----> 3\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
      "\u001b[1;31mRuntimeError\u001b[0m: dot : expected both vectors to have same dtype, but found Double and Float"
     ]
    }
   ],
   "source": [
    "z = wSum(X[0,:],W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights \n",
    "![](Weights.png)\n",
    "\n",
    "### Every neuron incoming input plus 1 for baise is the total no of weights on that individual neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardStep(X,W_list):\n",
    "    h = torch.from_numpy(X)\n",
    "    for W in W_list:\n",
    "        z = torch.matmul(W,h)\n",
    "        h = activate(z)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[205], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m W_list\u001b[38;5;241m.\u001b[39mappend(W2)\n\u001b[0;32m     13\u001b[0m W_list\u001b[38;5;241m.\u001b[39mappend(W3)\n\u001b[1;32m---> 14\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mforwardStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mW_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(z)\n",
      "Cell \u001b[1;32mIn[204], line 2\u001b[0m, in \u001b[0;36mforwardStep\u001b[1;34m(X, W_list)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforwardStep\u001b[39m(X,W_list):\n\u001b[1;32m----> 2\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m W \u001b[38;5;129;01min\u001b[39;00m W_list:\n\u001b[0;32m      4\u001b[0m         z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(W,h)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "inputDim = 10\n",
    "n = 1000\n",
    "X = np.random.rand(n,inputDim)\n",
    "y = np.random.randint(0,2,n)\n",
    "\n",
    "W1 = torch.tensor(np.random.uniform(0,1,(2,inputDim)),requires_grad=True)\n",
    "W2 = torch.tensor(np.random.uniform(0,1,(3,2)),requires_grad=True)\n",
    "W3 = torch.tensor(np.random.uniform(0,1,(1,3)),requires_grad=True)\n",
    "\n",
    "W_list = []\n",
    "W_list.append(W1)\n",
    "W_list.append(W2)\n",
    "W_list.append(W3)\n",
    "z = forwardStep(X[0,:],W_list)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_hat,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Decent and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(x):\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParams(W_list, dW_list, lr):\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(W_list)):\n",
    "            W_list[i] -= lr * dW_list[i]\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN_sgd(X, y, W_list, loss_fn, lr=0.0001, nepochs=100):\n",
    "    for epoch in range(nepochs):\n",
    "        avgLoss = []\n",
    "        for i in range(len(y)):\n",
    "            Xin = X[i, :]\n",
    "            yTrue = y[i]\n",
    "            y_hat = forwardStep(Xin, W_list)\n",
    "            loss = loss_fn(y_hat, torch.tensor(yTrue, dtype=torch.double))\n",
    "            loss.backward()\n",
    "            avgLoss.append(loss.item())\n",
    "            sys.stdout.flush()\n",
    "            dW_list = []\n",
    "            for j in range(len(W_list)):\n",
    "                dW_list.append(W_list[j].grad.data)\n",
    "            W_list = updateParams(W_list, dW_list, lr)\n",
    "            for j in range(len(W_list)):\n",
    "                W_list[j].grad.data.zero_()\n",
    "        print(\"Loss after epoch=%d: %f\" % (epoch, np.mean(np.array(avgLoss))))\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](https://mermaid.ink/img/pako:eNplkctygjAUhl8lc9bogIhSFu1U8dppN7aLFlycgRQySsKEMF5590LE6rRZJef7_uQkOUEkYgoeJBLzlLz7ISf1eA5WCqVak07nkYyCBWeK4ZYdKdlRlqSqWF-8kRbGwVRIQjFKCc1FlLZwrKF_g0oi44wnhO4xy7e09XztTRpvhzImuRQ5JqiY4K0x0cY0GIssLxUlW1FcO5hqNAtGGG3-B2eazoOPPEb1t_m5hovTq5D02lLxVF3gooHnT1qciX9feRNnsmwjzV1_A8tbYHxfaQIvwYTH9algQEZlhiyuX_zUWCGolGY0BK-exig3IYS8qj0slVgdeASekiU1oNQ38BnWH5WB943boq7myL-EyK5SvQTvBHvwnJ7dNQcPpuValjN0B44BB_Bss-v2e647tPrWwHT6tlsZcNQbmN0Ha9CzXccaOqbdd-2eAVKUSdqeVf0AGhOoNg?type=png)](https://mermaid.live/edit#pako:eNplkctygjAUhl8lc9bogIhSFu1U8dppN7aLFlycgRQySsKEMF5590LE6rRZJef7_uQkOUEkYgoeJBLzlLz7ISf1eA5WCqVak07nkYyCBWeK4ZYdKdlRlqSqWF-8kRbGwVRIQjFKCc1FlLZwrKF_g0oi44wnhO4xy7e09XztTRpvhzImuRQ5JqiY4K0x0cY0GIssLxUlW1FcO5hqNAtGGG3-B2eazoOPPEb1t_m5hovTq5D02lLxVF3gooHnT1qciX9feRNnsmwjzV1_A8tbYHxfaQIvwYTH9algQEZlhiyuX_zUWCGolGY0BK-exig3IYS8qj0slVgdeASekiU1oNQ38BnWH5WB943boq7myL-EyK5SvQTvBHvwnJ7dNQcPpuValjN0B44BB_Bss-v2e647tPrWwHT6tlsZcNQbmN0Ha9CzXccaOqbdd-2eAVKUSdqeVf0AGhOoNg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN_batch(X,y,W_list,loss_fn,lr=0.0001,nepochs=100):\n",
    "    n = len(y)\n",
    "    for epoch in range(nepochs):\n",
    "        loss = 0\n",
    "        for i in range(n):\n",
    "            Xin = X[i,:]\n",
    "            yTrue = y[i]\n",
    "            y_hat = forwardStep(Xin,W_list)\n",
    "            loss += loss_fn(y_hat,torch.tensor(yTrue,dtype=torch.double))\n",
    "        loss = loss/n\n",
    "        loss.backward()\n",
    "        sys.stdout.flush()\n",
    "        dW_list = []\n",
    "        for j in range(len(W_list)):\n",
    "            dW_list.append(W_list[j].grad.data)\n",
    "        W_list = updateParams(W_list,dW_list,lr)\n",
    "        for j in range(len(W_list)):\n",
    "            W_list[j].grad.data.zero_()\n",
    "        print(\"Loss after epoch=%d: %f\" %(epoch,loss))\n",
    "    return W_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN_minibatch(X,y,W_list,loss_fn,lr=0.0001,nepochs=100,batchSize=16):\n",
    "    n = len(y)\n",
    "    numBatches = n//batchSize\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "        for batch in range(numBatches):\n",
    "            X_batch = X[batch*batchSize:(batch+1)*batchSize,:]\n",
    "            y_batch = y[batch*batchSize:(batch+1)*batchSize]\n",
    "            loss = 0\n",
    "            for i in range(batchSize):\n",
    "                Xin = X_batch[i,:]\n",
    "                yTrue = y_batch[i]\n",
    "                y_hat = forwardStep(Xin,W_list)\n",
    "                loss += loss_fn(y_hat,torch.tensor(yTrue,dtype=torch.double))\n",
    "            loss = loss/batchSize\n",
    "            loss.backward()\n",
    "            sys.stdout.flush()\n",
    "            dW_list = []\n",
    "            for j in range(len(W_list)):\n",
    "                dW_list.append(W_list[j].grad.data)\n",
    "            W_list = updateParams(W_list,dW_list,lr)\n",
    "            for j in range(len(W_list)):\n",
    "                W_list[j].grad.data.zero_()\n",
    "        print(\"Loss after epoch=%d: %f\" %(epoch,loss/numBatches))\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 10\n",
    "n = 1000\n",
    "X = np.random.rand(n,inputDim)\n",
    "y = np.random.randint(0,2,n)\n",
    "\n",
    "W1 = torch.tensor(np.random.uniform(0,1,(2,inputDim)),requires_grad=True)\n",
    "W2 = torch.tensor(np.random.uniform(0,1,(3,2)),requires_grad=True)\n",
    "W3 = torch.tensor(np.random.uniform(0,1,3),requires_grad=True)\n",
    "\n",
    "W_list = []\n",
    "W_list.append(W1)\n",
    "W_list.append(W2)\n",
    "W_list.append(W3)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "W_list = trainNN_sgd(X,y,W_list,loss_fn,lr=0.0001,nepochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss_fun = nn.BCELoss\n",
    "lr = 0.0001\n",
    "x = torch.randn(1)\n",
    "y = torch.randint(0,2,(0,1), dtype = torch.float)\n",
    "w = torch.randn(1, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nIter = 100\n",
    "for i in range(nIter):\n",
    "    y_hat = m(w*x)\n",
    "    loss = loss_fun(y_hat,y)\n",
    "    loss.backward()\n",
    "    dw = w.grad.data\n",
    "    with torch.no_grad():\n",
    "        w -= lr*dw\n",
    "    w.grad.data.zero_()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural network has non-linear activation functions. then its called as neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](MDP.png)\n",
    "\n",
    "framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACTIVATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_fun = nn.Sigmoid()\n",
    "x = 100*torch.randn(1)\n",
    "print(x,activation_fun(x))\n",
    "\n",
    "# todo the bigger the number sigmoid will output the closer to 1 and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_fun = nn.ReLU()\n",
    "x = 100*torch.randn(1)\n",
    "print(x,activation_fun(x))\n",
    "\n",
    "# todo if the number is negative it will output 0 and if it is positive it will output the number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_fun = nn.Sigmoid()\n",
    "x = torch.randn(1)\n",
    "y = torch.randint(0,2,(1,),dtype=torch.float)\n",
    "y_hat = activation_fun(x)\n",
    "loss_fun = nn.BCELoss()\n",
    "loss_value = loss_fun(y_hat,y)\n",
    "print(loss_value.item())\n",
    "\n",
    "# todo if the number is negative it will output 0 and if it is positive it will output the number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYTORCH IMPLEMENTATION OF NEURAL NETWORK FULLY CONNECTED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 10\n",
    "n = 1000\n",
    "X = np.random.rand(n,inputDim)\n",
    "y = np.random.randint(0,2,n)\n",
    "\n",
    "tensor_x = torch.Tensor(X)\n",
    "tensor_y = torch.Tensor(y)\n",
    "Xy = TensorDataset(tensor_x,tensor_y)\n",
    "Xy_loader = DataLoader(Xy,batch_size=16,shuffle=True,drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(inputDim,200),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(num_features=200),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(200,100),\n",
    "    nn.Tanh(),\n",
    "    nn.BatchNorm1d(num_features=100),\n",
    "    nn.Linear(100,1),\n",
    "    nn.Sigmoid()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6855936050415039\n",
      "0.7457056045532227\n",
      "0.632402241230011\n",
      "0.73485267162323\n",
      "0.7375826239585876\n",
      "0.7483521699905396\n",
      "0.6166559457778931\n",
      "0.6985070705413818\n",
      "0.6339077353477478\n",
      "0.7038643956184387\n",
      "0.6778419017791748\n",
      "0.7039530277252197\n",
      "0.6300564408302307\n",
      "0.6448913216590881\n",
      "0.7625095844268799\n",
      "0.7205974459648132\n",
      "0.6668219566345215\n",
      "0.720833420753479\n",
      "0.7404969930648804\n",
      "0.6569193601608276\n",
      "0.5139963626861572\n",
      "0.6820937395095825\n",
      "0.5406128168106079\n",
      "0.6147273182868958\n",
      "0.6223942637443542\n",
      "0.5815938711166382\n",
      "0.6043765544891357\n",
      "0.6376553773880005\n",
      "0.7194736003875732\n",
      "0.590983510017395\n",
      "0.7603735327720642\n",
      "0.47641080617904663\n",
      "0.7361555099487305\n",
      "0.5252870321273804\n",
      "0.5708986520767212\n",
      "0.5991623997688293\n",
      "0.6995691061019897\n",
      "0.6001617908477783\n",
      "0.5062424540519714\n",
      "0.7035475969314575\n",
      "0.5171946287155151\n",
      "0.6610448956489563\n",
      "0.5423425436019897\n",
      "0.7610658407211304\n",
      "0.6877316236495972\n",
      "0.5319892764091492\n",
      "0.5198844075202942\n",
      "0.5883615612983704\n",
      "0.6779106259346008\n",
      "0.521933376789093\n",
      "0.921233057975769\n",
      "0.48555877804756165\n",
      "0.5823866128921509\n",
      "0.4845433831214905\n",
      "0.48352253437042236\n",
      "0.6259982585906982\n",
      "0.559937059879303\n",
      "0.7980818748474121\n",
      "0.4412740170955658\n",
      "0.6176831126213074\n",
      "0.474523663520813\n",
      "0.4234522581100464\n",
      "0.5853739976882935\n",
      "0.6394476890563965\n",
      "0.5552805662155151\n",
      "0.5536530017852783\n",
      "0.496990442276001\n",
      "0.5246111750602722\n",
      "0.6878983378410339\n",
      "0.5197948813438416\n",
      "0.5697683095932007\n",
      "0.5878561735153198\n",
      "0.7252643704414368\n",
      "0.6124188303947449\n",
      "0.47841310501098633\n",
      "0.4852765202522278\n",
      "0.3627258241176605\n",
      "0.480768084526062\n",
      "0.8272051811218262\n",
      "0.7353879809379578\n",
      "0.7379227876663208\n",
      "0.42614322900772095\n",
      "0.4848962128162384\n",
      "0.5487439632415771\n",
      "0.5443442463874817\n",
      "0.4885721802711487\n",
      "0.4133325517177582\n",
      "0.37308835983276367\n",
      "0.6686198115348816\n",
      "0.5690536499023438\n",
      "0.5625674724578857\n",
      "0.6168989539146423\n",
      "0.6042306423187256\n",
      "0.5314303636550903\n",
      "0.5292532444000244\n",
      "0.5768966674804688\n",
      "0.5129722952842712\n",
      "0.6928585767745972\n",
      "0.3870907127857208\n",
      "0.5391080975532532\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Ensure NumPy is available\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError:\n",
    "    raise RuntimeError(\"Numpy is not available\")\n",
    "\n",
    "nepochs = 100\n",
    "for epoch in range(nepochs):\n",
    "    for X, y in Xy_loader:\n",
    "        batch_size = X.shape[0]\n",
    "        y_hat = model(X.view(batch_size, -1))\n",
    "        \n",
    "        # Reshape y to match y_hat's shape\n",
    "        y = y.view_as(y_hat)\n",
    "        \n",
    "        loss = loss_fn(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(float(loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with torch.no_grad():\n",
    "    xt = torch.tensor(np.random.rand(1,inputDim))\n",
    "    y2 = model(xt.float())\n",
    "    print(y2.detach().numpy()[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
