{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "- Max(state,action)\n",
    "\n",
    "```\n",
    "max_new_state = np.max(q_table[new_state,:])\n",
    "\n",
    "q_table[state, action] = (1 - alpha) * q_table[state, action] + alpha * (reward + gamma * new_state_max - q_table[state, action])\n",
    "```\n",
    "\n",
    "# SARSA -> (State,Action,Reward,State2,Action2)\n",
    "- Next Action\n",
    "\n",
    "```\n",
    "sarsa_new_state = q_table(new_state,action2)\n",
    "\n",
    "q_table[state, action] = (1 - alpha) * q_table[state, action] + alpha * (reward + gamma * sarsa_new_state - q_table[state, action])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "### OFF Policy: In this, the Learning agent learns the value function according to the action derived from another Policy\n",
    "\n",
    "# SARSA -> State-Action-Reward-State2-Action2\n",
    "### ON Policy: In this, the Learning agent learns the value function according to the current action derived from the  Policy currently being used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA VS Q-LEARNING\n",
    "\n",
    "| SARSA (On Policy)                    | Q-Learning (Off Policy)                |\n",
    "| ------------------------------------ | -------------------------------------- |\n",
    "| Slow                                 | Fast                                   |\n",
    "| Relatively less risk of falling      | Relatively high risk of falling        |\n",
    "| Use if cost of error is quite high   | Use if cost of error is not verry high |\n",
    "| EX: Self-driving (the error is high) | EX: Chess (the error is low)           |\n",
    "| Increase in Episodes will increase the accuracy of sarsa as its On Policy Learning | Increase in Episodes will also increase the accuracy of Q-Learning but after a point it will saturate |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False,render_mode=\"ansi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_size =  env.action_space.n\n",
    "state_space_size = env.observation_space.n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((state_space_size, action_space_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 20000\n",
    "learning_rate = 0.2\n",
    "max_steps = 100\n",
    "gamma = 0.99\n",
    "\n",
    "epsilon = 1\n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9193\n",
      "[[7.34887414e-01 9.46759887e-01 6.83664039e-01 6.84816585e-01]\n",
      " [6.83598065e-01 0.00000000e+00 5.42117648e-01 5.45481798e-01]\n",
      " [6.70929601e-01 8.11847235e-02 5.28655322e-02 8.09794681e-02]\n",
      " [1.49778143e-01 0.00000000e+00 3.77530996e-05 9.12362724e-06]\n",
      " [7.73715437e-01 9.59741089e-01 0.00000000e+00 7.73314991e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 9.80073426e-01 0.00000000e+00 2.74639858e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [7.75284392e-01 0.00000000e+00 9.69944408e-01 8.06340670e-01]\n",
      " [9.00493251e-01 7.84080000e-01 9.79992679e-01 0.00000000e+00]\n",
      " [9.70076887e-01 9.89983129e-01 0.00000000e+00 9.70131067e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 9.79866798e-01 9.90000000e-01 9.64623434e-01]\n",
      " [9.80085699e-01 9.89930335e-01 1.00000000e+00 9.80048462e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    state, _ = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        if random.uniform(0, 1) > epsilon:\n",
    "            action = np.argmax(q_table[state, :])  # Exploit\n",
    "        else:\n",
    "            action = env.action_space.sample()  # Explore\n",
    "\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if random.uniform(0, 1) > epsilon:\n",
    "            new_action = np.argmax(q_table[new_state, :])  # Exploit\n",
    "        else:\n",
    "            new_action = env.action_space.sample()  # Explore\n",
    "\n",
    "        sarsa_new_state = np.max(q_table[new_state, new_action])\n",
    "\n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (\n",
    "            reward + gamma * sarsa_new_state - q_table[state, action]\n",
    "        )\n",
    "        total_rewards += reward\n",
    "\n",
    "        state = new_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print(\"Score:\", str(sum(rewards) / total_episodes))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI GENERATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9228\n",
      "[[0.73661021 0.89355217 0.75394609 0.73725602]\n",
      " [0.87716641 0.         0.74896271 0.74873778]\n",
      " [0.74967779 0.79421415 0.74985245 0.74809438]\n",
      " [0.74964372 0.         0.49552629 0.57573266]\n",
      " [0.82658133 0.90257795 0.         0.77851939]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.98004468 0.         0.83413197]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.72456904 0.         0.94425273 0.72263016]\n",
      " [0.69852575 0.9801     0.78243454 0.        ]\n",
      " [0.96668349 0.98900533 0.         0.96655416]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.97157932 0.99       0.9419982 ]\n",
      " [0.97949867 0.99       1.         0.97510874]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    state, _ = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "\n",
    "    # Choose action from state using epsilon-greedy policy\n",
    "    if random.uniform(0, 1) > epsilon:\n",
    "        action = np.argmax(q_table[state, :])  # Exploit\n",
    "    else:\n",
    "        action = env.action_space.sample()  # Explore\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Choose next action from new_state using epsilon-greedy policy\n",
    "        if random.uniform(0, 1) > epsilon:\n",
    "            new_action = np.argmax(q_table[new_state, :])  # Exploit\n",
    "        else:\n",
    "            new_action = env.action_space.sample()  # Explore\n",
    "\n",
    "        # SARSA update rule\n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (\n",
    "            reward + gamma * q_table[new_state, new_action] - q_table[state, action]\n",
    "        )\n",
    "        total_rewards += reward\n",
    "\n",
    "        state = new_state\n",
    "        action = new_action  # Update action to the new action\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "print(\"Score:\", str(sum(rewards) / total_episodes))\n",
    "print(q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
