{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "rootdir = './Data_cfar10'\n",
    "T = datasets.CIFAR10(rootdir,train=True,download=True)\n",
    "V = datasets.CIFAR10(rootdir,train=False,download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.cifar.CIFAR10"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = T[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD1AxqMnio4ZILhN8Esci+qMCK5248TR/8ACGPfJuac27DO0kbwMHJ/WvKfDfiS60jXZPsL7ogoIRsgSAjuPTNdbrrTzOCOHckz3zyx6VWvJI7eNWcgBmCjPqa4lfia/lOBpRaaNNzAS4Xp+dcbr3i/VdZ2zF/Itp3jCRRNnawyRn0PWlLExWxX1We7R00V7baYsNpKXk0sswnh2Fid2QMd+pFcX4xews4rS60dpTEoKMZYSuAeQpPc5zXbSeQ4MchAwO/p61Xv7Ox1y18hmGFZXHHHBzXkqrrqdEatlY8+0nRPEEyIyPDHDMoYSPIDhTz0HNNsrS7i8TR6PMyP9lk8zeDjcoBI4/EV6Rp9gLGyjghVWWIFeOuO1Z17oYk16LVkC+coRTnuvIP6EU/aJt3H7aT0P//Z",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'horse'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.classes[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X).shape\n",
    "#todo converting the image to a 1D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*32*3\n",
    "#todo  after multiplication of 32*32*3 we get 3072 which is the inputdimenion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "T = datasets.CIFAR10(rootdir,train=True,download=True,\n",
    "                    transform=transforms.ToTensor())\n",
    "V = datasets.CIFAR10(rootdir,train=False,download=True,\n",
    "                    transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tL = torch.utils.data.DataLoader(T,batch_size=64,shuffle=True,drop_last=True)\n",
    "vL = torch.utils.data.DataLoader(V,batch_size=64,shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3072,100),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(100,10),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(tL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1837.3052978515625\n",
      "1799.118408203125\n",
      "1799.1370849609375\n",
      "1799.1463623046875\n",
      "1799.2218017578125\n"
     ]
    }
   ],
   "source": [
    "nepochs = 5\n",
    "for e in range(nepochs):\n",
    "    eLoss = 0\n",
    "    for X,y in tL:\n",
    "        batch_size = X.shape[0]\n",
    "        y_hat = model(X.view(batch_size,-1))\n",
    "        loss = loss_fn(y_hat,y)\n",
    "        eLoss += loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    print(float(eLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0999599358974359\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "t = 0\n",
    "with torch.no_grad():\n",
    "    for Xv,yv in vL:\n",
    "        batch_size = Xv.shape[0]\n",
    "        y_hat = model(Xv.view(batch_size,-1))\n",
    "        _,p = torch.max(y_hat,dim=1)\n",
    "        t+=yv.shape[0]\n",
    "        c+=int((p==yv).sum())\n",
    "print(c/t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
